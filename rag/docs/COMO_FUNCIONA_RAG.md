# Como Funciona o RAG: Chunks e Embeddings

Este documento explica em detalhes como funciona o processo de chunking (divis√£o) e embeddings (vetoriza√ß√£o) no sistema RAG implementado neste projeto.

## √çndice
1. [Vis√£o Geral](#vis√£o-geral)
2. [Chunking - Divis√£o em Peda√ßos](#chunking---divis√£o-em-peda√ßos)
3. [Embeddings - Vetoriza√ß√£o](#embeddings---vetoriza√ß√£o)
4. [Fluxo Completo](#fluxo-completo)
5. [Implementa√ß√£o no C√≥digo](#implementa√ß√£o-no-c√≥digo)
6. [M√©tricas e Performance](#m√©tricas-e-performance)
7. [Par√¢metros Ajust√°veis](#par√¢metros-ajust√°veis)
8. [Testando o Processo](#testando-o-processo)

---

## Vis√£o Geral

O sistema RAG (Retrieval Augmented Generation) funciona em duas etapas principais:

1. **Indexa√ß√£o (uma vez)**: Processar documentos e armazen√°-los
2. **Busca (cada pergunta)**: Encontrar informa√ß√µes relevantes

O processo de **chunking** e **embeddings** √© o cora√ß√£o da indexa√ß√£o.

---

## Chunking - Divis√£o em Peda√ßos

### O Que √â?

Chunking √© o processo de dividir documentos grandes em peda√ßos menores (chunks) que podem ser processados e buscados individualmente.

### Por Que Usar Chunks?

- **Documentos grandes** n√£o cabem na mem√≥ria do modelo de embedding
- **Chunks menores** = buscas mais precisas e espec√≠ficas
- **Overlap (sobreposi√ß√£o)** mant√©m o contexto entre chunks consecutivos

### Implementa√ß√£o

O processo est√° implementado no m√©todo `_split_text()` em [rag/rag_module.py:359-390](rag/rag_module.py#L359-L390):

```python
def _split_text(self, text: str, chunk_size: int = 500, overlap: int = 50) -> List[str]:
    """Divide texto em chunks com overlap para manter contexto"""
    words = text.split()
    chunks = []
    i = 0

    while i < len(words):
        # Pega chunk_size palavras
        chunk_words = words[i:i + chunk_size]
        chunk = ' '.join(chunk_words)

        if len(chunk.strip()) > 0:
            chunks.append(chunk.strip())

        # Move para pr√≥ximo chunk com overlap
        i += chunk_size - overlap

    return chunks if chunks else [text.strip()]
```

### Configura√ß√£o Padr√£o

Definida em [rag/rag_config.py:13-14](rag/rag_config.py#L13-L14):

- **chunk_size**: 500 palavras (~2000 caracteres)
- **chunk_overlap**: 50 palavras (10% de sobreposi√ß√£o)

### Exemplo Pr√°tico

```
Documento: "A empresa XPTO oferece suporte t√©cnico... [1000 palavras]"

Chunk 1: palavras 0-500
Chunk 2: palavras 450-950 (overlap de 50 palavras com Chunk 1)
Chunk 3: palavras 900-1000 (overlap de 50 palavras com Chunk 2)
```

### Visualiza√ß√£o do Chunking

```
üìÑ guia_resolucao_problemas.txt (200 linhas = ~2400 palavras)
‚îÇ
‚îú‚îÄ Chunk 0 (palavras 0-500)
‚îÇ  Texto: "Para resolver problemas de conex√£o..."
‚îÇ
‚îú‚îÄ Chunk 1 (palavras 450-950)  ‚Üê 50 palavras repetidas
‚îÇ  Texto: "...de conex√£o, verifique os cabos..."
‚îÇ
‚îú‚îÄ Chunk 2 (palavras 900-1400)  ‚Üê 50 palavras repetidas
‚îÇ  Texto: "...os cabos e reinicie o modem..."
‚îÇ
‚îî‚îÄ Chunk 3 (palavras 1350-1850)
   Texto: "...o modem. Se o problema persistir..."
```

---

## Embeddings - Vetoriza√ß√£o

### O Que S√£o Embeddings?

Embeddings s√£o representa√ß√µes num√©ricas (vetores) que capturam o **significado sem√¢ntico** do texto.

- Um chunk de texto vira um vetor de **384 n√∫meros**
- Textos com significados similares t√™m vetores pr√≥ximos
- Permite busca por **similaridade sem√¢ntica**, n√£o apenas palavras-chave

### Modelo Utilizado

**Sentence Transformers**: `paraphrase-multilingual-MiniLM-L12-v2`

- **Multil√≠ngue**: Suporta portugu√™s, ingl√™s e 50+ idiomas
- **Dimens√µes**: 384 (cada embedding tem 384 n√∫meros)
- **Tamanho**: ~400MB (baixado na primeira execu√ß√£o)
- **Performance**: ~50ms por chunk

### Como Funciona

Implementado em [rag/rag_module.py:424-432](rag/rag_module.py#L424-L432):

```python
# Para cada chunk
for i, chunk in enumerate(chunks):
    # Gera embedding (vetor 384 dimens√µes)
    embedding = self.embedding_model.encode(chunk).tolist()

    # Armazena no Qdrant
    point = PointStruct(
        id=doc_count,
        vector=embedding,  # Vetor [0.123, -0.456, ...]
        payload={
            "text": chunk,
            "source": file.name,
            "category": category,
            "chunk_index": i
        }
    )
```

### Exemplo Visual

```
Texto: "Como resolver problema de internet?"
‚Üì (Sentence Transformers)
Embedding: [0.234, -0.123, 0.456, ..., 0.789]  (384 n√∫meros)

Texto: "Solu√ß√£o para conex√£o sem fio"
‚Üì
Embedding: [0.221, -0.115, 0.443, ..., 0.772]  (vetores similares!)

Texto: "Receita de bolo de chocolate"
‚Üì
Embedding: [-0.456, 0.789, -0.123, ..., 0.234]  (vetor diferente)
```

### Busca por Similaridade

A busca usa **cosine similarity** (similaridade cosseno) entre vetores:

```
Query: "problema de internet"
Embedding da query: [0.234, -0.123, 0.456, ...]

Comparar com todos os chunks no banco:
‚îú‚îÄ Chunk 12: similarity = 0.87 ‚úÖ (muito relevante)
‚îú‚îÄ Chunk 5:  similarity = 0.72 ‚úÖ (relevante)
‚îú‚îÄ Chunk 23: similarity = 0.65 ‚úÖ (relevante)
‚îú‚îÄ Chunk 8:  similarity = 0.42 ‚ùå (pouco relevante)
‚îî‚îÄ Chunk 15: similarity = 0.15 ‚ùå (irrelevante)

Retorna top 3 chunks com score > 0.5
```

---

## Fluxo Completo

### 1. Indexa√ß√£o (Executada Uma Vez)

```
üìÑ Documento (TXT/PDF)
‚îÇ
‚îú‚îÄ 1. Ler arquivo
‚îÇ  ‚îî‚îÄ PyPDF2 (PDF) ou open() (TXT)
‚îÇ
‚îú‚îÄ 2. Dividir em chunks
‚îÇ  ‚îî‚îÄ _split_text(text, chunk_size=500, overlap=50)
‚îÇ     ‚îú‚îÄ Chunk 0: "Para resolver problemas..."
‚îÇ     ‚îú‚îÄ Chunk 1: "...problemas de conex√£o..."
‚îÇ     ‚îî‚îÄ Chunk 2: "...conex√£o, verifique..."
‚îÇ
‚îú‚îÄ 3. Gerar embeddings
‚îÇ  ‚îî‚îÄ embedding_model.encode(chunk)
‚îÇ     ‚îú‚îÄ Chunk 0 ‚Üí [0.123, -0.456, ...]
‚îÇ     ‚îú‚îÄ Chunk 1 ‚Üí [0.234, -0.567, ...]
‚îÇ     ‚îî‚îÄ Chunk 2 ‚Üí [0.345, -0.678, ...]
‚îÇ
‚îî‚îÄ 4. Armazenar no Qdrant
   ‚îî‚îÄ qdrant_client.upsert()
      ‚îú‚îÄ Vector: [0.123, -0.456, ...]
      ‚îî‚îÄ Payload: {text, source, category, chunk_index}
```

### 2. Busca (A Cada Pergunta do Usu√°rio)

```
üîç Pergunta: "Como resolver problema de internet?"
‚îÇ
‚îú‚îÄ 1. Gerar embedding da pergunta
‚îÇ  ‚îî‚îÄ query_vector = [0.221, -0.442, ...]
‚îÇ
‚îú‚îÄ 2. Buscar chunks similares no Qdrant
‚îÇ  ‚îî‚îÄ qdrant_client.search(query_vector, top_k=3)
‚îÇ     ‚îú‚îÄ Chunk 12 (score: 0.87) ‚úÖ
‚îÇ     ‚îú‚îÄ Chunk 5 (score: 0.72) ‚úÖ
‚îÇ     ‚îî‚îÄ Chunk 23 (score: 0.65) ‚úÖ
‚îÇ
‚îú‚îÄ 3. Formatar contexto
‚îÇ  ‚îî‚îÄ "Contexto relevante:\n[Chunk 12]\n[Chunk 5]\n[Chunk 23]"
‚îÇ
‚îî‚îÄ 4. Enviar para LLM (GPT)
   ‚îî‚îÄ Prompt: "Com base no contexto: ... responda: ..."
      ‚îî‚îÄ Resposta: "Para resolver problemas de internet, siga..."
```

---

## Implementa√ß√£o no C√≥digo

### Arquivos Principais

1. **[rag/rag_module.py](rag/rag_module.py)** - Motor RAG
2. **[rag/rag_config.py](rag/rag_config.py)** - Configura√ß√µes
3. **[app_01.py](app_01.py)** - Integra√ß√£o com Streamlit

### Inicializa√ß√£o do Modelo

Em [rag/rag_module.py:86-91](rag/rag_module.py#L86-L91):

```python
self.embedding_model = SentenceTransformer(
    'paraphrase-multilingual-MiniLM-L12-v2',
    device='cpu'
)
```

### Carregamento de Documentos

Em [rag/rag_module.py:391-446](rag/rag_module.py#L391-L446):

```python
def load_documents(self, directory: str, category_filter: Optional[str] = None):
    """
    Carrega documentos, divide em chunks e gera embeddings

    Processo:
    1. Escanear diret√≥rio recursivamente
    2. Ler TXT/PDF
    3. Dividir em chunks (500 palavras, overlap 50)
    4. Gerar embeddings (384 dimens√µes)
    5. Salvar no Qdrant
    """
```

### Busca Sem√¢ntica

Em [rag/rag_module.py:267-319](rag/rag_module.py#L267-L319):

```python
def retrieve(self, query: str, top_k: int = 3, score_threshold: float = 0.5):
    """
    Busca chunks relevantes para a query

    Processo:
    1. Gerar embedding da query
    2. Buscar vetores similares no Qdrant (cosine similarity)
    3. Filtrar por score_threshold
    4. Retornar top_k resultados
    """

    # Gerar embedding da query
    query_vector = self.embedding_model.encode(query).tolist()

    # Buscar no Qdrant
    results = self.qdrant_client.search(
        collection_name=self.collection_name,
        query_vector=query_vector,
        limit=top_k,
        score_threshold=score_threshold
    )

    return results
```

---

## M√©tricas e Performance

### Primeira Execu√ß√£o

- **Download do modelo**: ~400MB (uma vez, fica em cache)
- **Processamento**: ~2 minutos para 4 documentos (~50KB texto)
- **Chunks gerados**: ~1200 chunks (4 docs √ó ~300 chunks cada)
- **Storage Qdrant**: ~5MB
- **RAM necess√°ria**: ~800MB

### Execu√ß√µes Seguintes

- **Modelo**: J√° est√° em cache
- **Tempo de busca**: 50-200ms por query
- **RAM em uso**: ~600MB (modelo carregado)
- **Inicializa√ß√£o**: ~3 segundos

### Estat√≠sticas Exemplo

```python
rag = create_rag_instance('./rag/base_conhecimento')
stats = rag.get_stats()

{
    'total_documents': 1247,  # Total de chunks
    'categories': {
        'suporte_tecnico': 623,
        'relacionamento': 624
    },
    'sources': {
        'guia_resolucao_problemas.txt': 312,
        'procedimentos_seguranca.txt': 311,
        'politicas_atendimento.txt': 312,
        'gestao_conflitos.txt': 312
    }
}
```

---

## Par√¢metros Ajust√°veis

### Configura√ß√£o em [rag/rag_config.py](rag/rag_config.py)

```python
RAG_CONFIG = {
    "enabled": True,
    "knowledge_base_dir": "./rag/base_conhecimento",
    "persist_path": "./rag/qdrant_storage",

    # Par√¢metros de chunking
    "chunk_size": 500,        # Tamanho do chunk (palavras)
    "chunk_overlap": 50,      # Overlap entre chunks

    # Par√¢metros de busca
    "default_top_k": 3,       # Quantos chunks retornar
    "score_threshold": 0.5,   # Score m√≠nimo (0-1)
}
```

### Como Ajustar

| Par√¢metro | Valor Menor | Valor Padr√£o | Valor Maior |
|-----------|-------------|--------------|-------------|
| **chunk_size** | 200-300<br>+ Busca precisa<br>+ Mais chunks<br>- Contexto fragmentado | 500 | 700-1000<br>+ Mais contexto<br>- Busca imprecisa<br>- Menos chunks |
| **chunk_overlap** | 20-30<br>+ Menos storage<br>- Perde contexto | 50 | 100-150<br>+ Preserva contexto<br>- Mais storage |
| **top_k** | 1-2<br>+ Resposta focada<br>+ Mais r√°pido | 3 | 5-10<br>+ Mais contexto<br>- Mais lento<br>- Pode confundir LLM |
| **score_threshold** | 0.3-0.4<br>+ Busca flex√≠vel<br>- Pode trazer irrelevantes | 0.5 | 0.7-0.8<br>+ Apenas muito relevantes<br>- Pode n√£o achar nada |

### Exemplos de Ajuste

```python
# Para documentos t√©cnicos muito detalhados
RAG_CONFIG["chunk_size"] = 700
RAG_CONFIG["chunk_overlap"] = 100

# Para respostas mais abrangentes
RAG_CONFIG["default_top_k"] = 5

# Para busca mais restritiva
RAG_CONFIG["score_threshold"] = 0.7
```

---

## Testando o Processo

### 1. Verificar Instala√ß√£o

```bash
python rag/utils/check_rag_setup.py
```

### 2. Testar Chunking

```python
from rag import QdrantRAG

# Criar inst√¢ncia
rag = QdrantRAG(persist_path='./rag/qdrant_storage')

# Testar divis√£o de texto
texto = "Este √© um texto de exemplo... " * 1000  # Texto longo
chunks = rag._split_text(texto, chunk_size=500, overlap=50)

print(f"Total de chunks: {len(chunks)}")
print(f"Tamanho chunk 1: {len(chunks[0].split())} palavras")
print(f"Overlap entre chunks: {len(set(chunks[0].split()) & set(chunks[1].split()))} palavras")
```

### 3. Testar Embeddings

```python
from rag import create_rag_instance

# Criar inst√¢ncia e carregar documentos
rag = create_rag_instance('./rag/base_conhecimento', verbose=True)

# Verificar estat√≠sticas
stats = rag.get_stats()
print(f"Total de chunks: {stats['total_documents']}")
print(f"Categorias: {stats['categories']}")

# Testar busca
docs = rag.retrieve('problema de internet', top_k=3)
for doc in docs:
    print(f"\nScore: {doc['score']:.2%}")
    print(f"Fonte: {doc['source']}")
    print(f"Texto: {doc['text'][:100]}...")
```

### 4. Testar Busca Sem√¢ntica

```python
from rag import create_rag_instance

rag = create_rag_instance('./rag/base_conhecimento')

# Testar diferentes queries
queries = [
    "Como resolver problema de internet?",
    "Prazo de devolu√ß√£o de produto",
    "Seguran√ßa de dados do cliente"
]

for query in queries:
    print(f"\n{'='*60}")
    print(f"Query: {query}")
    print('='*60)

    docs = rag.retrieve(query, top_k=3, score_threshold=0.5)

    for i, doc in enumerate(docs, 1):
        print(f"\n{i}. Score: {doc['score']:.2%} | {doc['source']}")
        print(f"   {doc['text'][:150]}...")
```

### 5. Ver Logs Detalhados

```python
import logging
logging.basicConfig(level=logging.INFO)

from rag import create_rag_instance

# Verbose mode mostra todos os passos
rag = create_rag_instance('./rag/base_conhecimento', verbose=True)
```

---

## Resumo Executivo

### O Que Acontece nos Bastidores

1. **Chunking** divide documentos grandes em peda√ßos gerenci√°veis (500 palavras) com overlap (50 palavras)
2. **Embeddings** transformam cada chunk em um vetor de 384 n√∫meros que captura o significado
3. **Qdrant** armazena esses vetores em um banco de dados otimizado para busca vetorial
4. **Busca sem√¢ntica** encontra chunks relevantes comparando vetores (cosine similarity)
5. **LLM** usa os chunks mais relevantes como contexto para gerar respostas precisas

### Benef√≠cios

- **Respostas precisas**: Baseadas em documentos reais da empresa
- **Escal√°vel**: Funciona com milhares de documentos
- **R√°pido**: Busca em 50-200ms
- **Multil√≠ngue**: Suporta portugu√™s nativamente
- **Sem√¢ntico**: Entende significado, n√£o apenas palavras-chave

### Processo Automatizado

Basta adicionar documentos em `rag/base_conhecimento/` e o sistema automaticamente:

1. L√™ o documento (TXT/PDF)
2. Divide em chunks
3. Gera embeddings
4. Armazena no Qdrant
5. Fica pronto para buscar!

### Exemplo Pr√°tico

```python
# 1. Adicionar documento
# Copiar arquivo para: rag/base_conhecimento/suporte_tecnico/novo_guia.txt

# 2. Carregar no sistema
from rag import create_rag_instance
rag = create_rag_instance('./rag/base_conhecimento')

# 3. Buscar automaticamente funciona!
docs = rag.retrieve('informa√ß√£o do novo guia', top_k=3)
```

---

## Refer√™ncias

- **C√≥digo fonte**: [rag/rag_module.py](rag/rag_module.py)
- **Configura√ß√µes**: [rag/rag_config.py](rag/rag_config.py)
- **Documenta√ß√£o completa**: [rag/docs/RAG_README.md](rag/docs/RAG_README.md)
- **Guia de instala√ß√£o**: [INSTALACAO_RAG.md](../INSTALACAO_RAG.md)
- **Quick Start**: [rag/docs/QUICK_START.md](rag/docs/QUICK_START.md)

---

**√öltima atualiza√ß√£o**: 2025-10-24
