"""
M√≥dulo RAG (Retrieval Augmented Generation) para VOXMAP
Vers√£o: 1.0

Sistema modular de busca sem√¢ntica usando Qdrant + Sentence Transformers
Suporta documentos TXT e PDF com processamento autom√°tico
"""

from qdrant_client import QdrantClient
from qdrant_client.models import Distance, VectorParams, PointStruct, MatchText
from sentence_transformers import SentenceTransformer
from pathlib import Path
from typing import List, Dict, Optional
import hashlib
import json
import re


class QdrantRAG:
    """
    Sistema RAG completo com Qdrant

    Funcionalidades:
    - Carregamento autom√°tico de TXT e PDF
    - Embeddings multil√≠ngue (PT-BR)
    - Busca por similaridade sem√¢ntica
    - Persist√™ncia em disco
    - Filtros por fonte/categoria
    """

    def __init__(
        self,
        knowledge_base_dir: str = "./base_conhecimento",
        collection_name: str = "voxmap_kb",
        persist_path: str = "./qdrant_storage",
        embedding_model: str = "paraphrase-multilingual-MiniLM-L12-v2",
        auto_load: bool = True,
        verbose: bool = True
    ):
        """
        Inicializa o sistema RAG

        Args:
            knowledge_base_dir: Pasta com documentos (TXT/PDF)
            collection_name: Nome da cole√ß√£o no Qdrant
            persist_path: Onde salvar o banco vetorial
            embedding_model: Modelo de embeddings (multil√≠ngue por padr√£o)
            auto_load: Se True, carrega documentos automaticamente
            verbose: Se True, mostra logs detalhados
        """
        self.knowledge_base_dir = knowledge_base_dir
        self.collection_name = collection_name
        self.verbose = verbose

        # Inicializa cliente Qdrant (persistente em disco)
        if self.verbose:
            print("üîß Inicializando Qdrant...")
        # self.client = QdrantClient(path=persist_path) - modo arquivo local
        self.client = QdrantClient(host="localhost", port=6333) # - modo servidor Docker

        # Carrega modelo de embeddings
        if self.verbose:
            print(f"üß† Carregando modelo de embeddings: {embedding_model}")
        self.embedding_model = SentenceTransformer(embedding_model)
        self.embedding_dim = self.embedding_model.get_sentence_embedding_dimension()

        # Cria cole√ß√£o se n√£o existir
        self._initialize_collection()

        # Carrega documentos automaticamente se solicitado
        if auto_load:
            if self.is_empty():
                if self.verbose:
                    print("üìö Base de conhecimento vazia. Carregando documentos...")
                self.load_documents(self.knowledge_base_dir)
            else:
                if self.verbose:
                    print(f"‚úÖ Base de conhecimento carregada: {self.count()} documentos")

    def __del__(self):
        """Cleanup do cliente Qdrant"""
        try:
            if hasattr(self, 'client') and self.client:
                self.client.close()
        except Exception:
            # Ignora erros de cleanup (comum com Qdrant local)
            pass

    def _initialize_collection(self):
        """Cria a cole√ß√£o no Qdrant se n√£o existir"""
        try:
            collections = self.client.get_collections().collections
            collection_exists = any(c.name == self.collection_name for c in collections)

            if not collection_exists:
                self.client.create_collection(
                    collection_name=self.collection_name,
                    vectors_config=VectorParams(
                        size=self.embedding_dim,
                        distance=Distance.COSINE
                    )
                )
                if self.verbose:
                    print(f"‚úÖ Cole√ß√£o '{self.collection_name}' criada")
        except Exception as e:
            if self.verbose:
                print(f"‚ö†Ô∏è  Erro ao inicializar cole√ß√£o: {e}")
            raise

    def _split_text(
        self,
        text: str,
        chunk_size: int = 50,
        overlap: int = 10
    ) -> List[str]:
        """
        Divide texto em chunks com overlap

        Args:
            text: Texto para dividir
            chunk_size: Tamanho aproximado (em palavras)
            overlap: Overlap entre chunks (em palavras)

        Returns:
            Lista de chunks
        """
        words = text.split()
        chunks = []

        i = 0
        while i < len(words):
            chunk_words = words[i:i + chunk_size]
            chunk = ' '.join(chunk_words)

            if len(chunk.strip()) > 0:
                chunks.append(chunk.strip())

            i += chunk_size - overlap

        return chunks if chunks else [text.strip()]

    def _load_txt(self, file_path: Path) -> str:
        """Carrega arquivo TXT"""
        with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
            return f.read()

    def _load_pdf(self, file_path: Path) -> str:
        """
        Carrega arquivo PDF usando PyPDF2
        Fallback para leitura b√°sica se PyPDF2 n√£o estiver dispon√≠vel
        """
        try:
            import PyPDF2

            text = []
            with open(file_path, 'rb') as f:
                reader = PyPDF2.PdfReader(f)
                for page in reader.pages:
                    page_text = page.extract_text()
                    if page_text:
                        text.append(page_text)

            return '\n\n'.join(text)

        except ImportError:
            if self.verbose:
                print(f"‚ö†Ô∏è  PyPDF2 n√£o instalado. Instale: pip install PyPDF2")
            return ""
        except Exception as e:
            if self.verbose:
                print(f"‚ö†Ô∏è  Erro ao ler PDF {file_path.name}: {e}")
            return ""

    def load_documents(
        self,
        dir_path: str,
        chunk_size: int = 500,  # <-- Usado apenas para PDFs
        file_types: List[str] = None
    ):
        """
        Carrega documentos de uma pasta

        Args:
            dir_path: Caminho da pasta
            chunk_size: Tamanho dos chunks (usado para PDFs)
            file_types: Lista de extens√µes (ex: ['.txt', '.pdf'])
        """
        if file_types is None:
            file_types = ['.txt', '.pdf']

        path = Path(dir_path)
        if not path.exists():
            if self.verbose:
                print(f"‚ö†Ô∏è  Pasta '{dir_path}' n√£o encontrada. Criando...")
            path.mkdir(parents=True, exist_ok=True)
            return

        points = []
        doc_count = self.count()  # ID inicial
        files_processed = 0

        # Processa cada tipo de arquivo
        for file_type in file_types:
            for file in path.glob(f"**/*{file_type}"):
                try:
                    chunks = []
                    content = ""

                    # Determina categoria pela estrutura de pastas
                    category = file.parent.name if file.parent.name != path.name else "geral"

<<<<<<< Updated upstream
=======
#                     # Carrega conte√∫do baseado na extens√£o
#                     if file_type == '.txt':
#                         content = self._load_txt(file)
#                         if not content or len(content.strip()) < 20: # Reduzido para 20
#                             if self.verbose:
#                                 print(f" ¬†‚äò {file.name}: vazio ou muito curto, ignorado")
#                             continue

#                         # --- L√ìGICA DE CHUNKING SEM√ÇNTICO PARA TXT ---
#                         # Usamos '---' como separador, baseado na estrutura do seu documento
#                         chunks_raw = content.split('PROBLEMA:')
#                         # Limpa os chunks: remove espa√ßos em branco no in√≠cio/fim
#                         chunks = [chunk.strip() for chunk in chunks_raw if chunk.strip()]
#                         # Filtra chunks que s√£o muito pequenos (ex: menos de 10 palavras)
#                         chunks = [chunk for chunk in chunks if len(chunk.split()) > 10]
#                         # --- FIM DA NOVA L√ìGICA ---
#                     import re
# # ... (O restante do seu rag_module.py)

# ... (Dentro da fun√ß√£o load_documents, no bloco if file_type == '.txt':)
>>>>>>> Stashed changes
                    if file_type == '.txt':
                        content = self._load_txt(file)
                        if not content or len(content.strip()) < 20:
                            if self.verbose:
                                print(f" ¬†‚äò {file.name}: vazio ou muito curto, ignorado")
                            continue

                        # --- L√ìGICA DE CHUNKING SEM√ÇNTICO APRIMORADO PARA TXT ---
                        
                        # Define os padr√µes de separa√ß√£o:
                        # 1. '===' seguido de texto e '===' (ex: === PROBLEMAS DE REDE ===)
                        # 2. 'PROBLEMA:' (ex: PROBLEMA: Computador n√£o liga)
                        # 3. '---' (linha divis√≥ria)
                        # Usamos '(?=...)' (lookahead) para *manter* o separador no in√≠cio do chunk.
                        # Ex: 'PROBLEMA: ' ser√° o in√≠cio do chunk, mantendo o contexto.
                        
                        split_pattern = r'(?=^=== [^=]+ ===|^PROBLEMA:|^---)'
                        
                        # Dividir o texto usando a regex (re.split)
                        # re.MULTILINE (m) garante que o '^' capture o in√≠cio de cada linha, n√£o s√≥ do arquivo
                        chunks_raw = re.split(split_pattern, content, flags=re.MULTILINE)

                        # Adicionar o separador 'PROBLEMA:' e '===' de volta aos chunks que o perderam
                        
                        # Limpa os chunks: remove espa√ßos em branco no in√≠cio/fim e remove vazios
                        chunks = [chunk.strip() for chunk in chunks_raw if chunk.strip()]
                        
                        # Filtra chunks que s√£o muito pequenos (ex: menos de 10 palavras)
                        chunks = [chunk for chunk in chunks if len(chunk.split()) > 10]
                        
                        # --- FIM DA NOVA L√ìGICA ---

                    elif file_type == '.pdf':
                        content = self._load_pdf(file)
                        if not content or len(content.strip()) < 50:
                            if self.verbose:
                                print(f" ¬†‚äò {file.name}: vazio ou muito curto, ignorado")
                            continue
                        
                        # --- L√ìGICA DIFERENTE PARA PDFS ---
                        # PDFs n√£o t√™m '---', ent√£o usamos o split por tamanho
                        chunks = self._split_text(content, chunk_size=chunk_size)
                        # --- FIM DA L√ìGICA ANTIGA ---

                    else:
                        continue
                    
                    if not chunks:
                        if self.verbose:
                            print(f" ¬†‚äò {file.name}: n√£o produziu chunks, ignorado")
                        continue

                    # Gera embeddings e cria pontos
                    for i, chunk in enumerate(chunks):
                        embedding = self.embedding_model.encode(chunk).tolist()

                        point = PointStruct(
                            id=doc_count,
                            vector=embedding,
                            payload={
                                "text": chunk,
                                "source": file.name,
                                "file_path": str(file.relative_to(path)),
                                "category": category,
                                "file_type": file_type,
                                "chunk_index": i,
                                "total_chunks": len(chunks)
                            }
                        )
                        points.append(point)
                        doc_count += 1

                    files_processed += 1
                    if self.verbose:
                        print(f" ¬†‚úì {file.name}: {len(chunks)} chunks ({category})")

                except Exception as e:
                    if self.verbose:
                        print(f" ¬†‚úó Erro ao processar {file.name}: {e}")

        # Insere todos os pontos (batch upsert)
        if points:
            batch_size = 100
            for i in range(0, len(points), batch_size):
                batch = points[i:i + batch_size]
                self.client.upsert(
                    collection_name=self.collection_name,
                    points=batch
                )

            if self.verbose:
                print(f"‚úÖ {files_processed} arquivos indexados ({len(points)} chunks)")
        else:
            if self.verbose:
                print("‚ö†Ô∏è  Nenhum documento v√°lido encontrado")

    def retrieve(
        self,
        query: str,
        top_k: int = 3,
        score_threshold: float = 0.5,
        category_filter: Optional[str] = None
    ) -> List[Dict]:
        """
        Busca documentos relevantes

        Args:
            query: Texto de busca
            top_k: N√∫mero de documentos a retornar
            score_threshold: Score m√≠nimo (0-1)
            category_filter: Filtrar por categoria espec√≠fica

        Returns:
            Lista de documentos com texto, fonte, score, etc.
        """
        try:
            # Gera embedding da query
            query_embedding = self.embedding_model.encode(query).tolist()

            # Prepara filtro se necess√°rio
            query_filter = None
            if category_filter:
                from qdrant_client.models import Filter, FieldCondition, MatchValue
                query_filter = Filter(
                    must=[
                        FieldCondition(
                            key="category",
                            match=MatchValue(value=category_filter)
                        )
                    ]
                )

            # Busca no Qdrant
            results = self.client.search(
                collection_name=self.collection_name,
                query_vector=query_embedding,
                limit=top_k,
                score_threshold=score_threshold,
                query_filter=query_filter
            )

            # Formata resultados
            documents = []
            for result in results:
                documents.append({
                    "text": result.payload["text"],
                    "source": result.payload["source"],
                    "category": result.payload.get("category", "geral"),
                    "file_type": result.payload.get("file_type", "txt"),
                    "score": round(result.score, 4),
                    "chunk_index": result.payload.get("chunk_index", 0)
                })

            return documents

        except Exception as e:
            if self.verbose:
                print(f"‚ö†Ô∏è  Erro na busca: {e}")
            return []
########################################################

    def get_categories(self) -> List[str]:
        """Retorna todas as categorias dispon√≠veis"""
        try:
            # Scroll por todos os pontos e coleta categorias √∫nicas
            categories = set()
            offset = None

            while True:
                records, offset = self.client.scroll(
                    collection_name=self.collection_name,
                    limit=100,
                    offset=offset,
                    with_payload=True,
                    with_vectors=False
                )

                for record in records:
                    cat = record.payload.get("category", "geral")
                    categories.add(cat)

                if offset is None:
                    break

            return sorted(list(categories))

        except Exception:
            return []

    def is_empty(self) -> bool:
        """Verifica se a cole√ß√£o est√° vazia"""
        return self.count() == 0

    def count(self) -> int:
        """Retorna n√∫mero de documentos"""
        try:
            info = self.client.get_collection(self.collection_name)
            return info.points_count
        except Exception:
            return 0

    def clear(self):
        """Limpa toda a cole√ß√£o"""
        try:
            self.client.delete_collection(self.collection_name)
            self._initialize_collection()
            if self.verbose:
                print("üóëÔ∏è  Base de conhecimento limpa")
        except Exception as e:
            if self.verbose:
                print(f"‚ö†Ô∏è  Erro ao limpar: {e}")

    def add_text(
        self,
        text: str,
        source: str = "manual",
        category: str = "geral",
        chunk_size: int = 500
    ):
        """
        Adiciona texto manualmente

        Args:
            text: Conte√∫do
            source: Nome da fonte
            category: Categoria do documento
            chunk_size: Tamanho dos chunks
        """
        chunks = self._split_text(text, chunk_size=chunk_size)
        points = []

        current_count = self.count()

        for i, chunk in enumerate(chunks):
            embedding = self.embedding_model.encode(chunk).tolist()

            point = PointStruct(
                id=current_count + i,
                vector=embedding,
                payload={
                    "text": chunk,
                    "source": source,
                    "category": category,
                    "file_type": "manual",
                    "chunk_index": i,
                    "total_chunks": len(chunks)
                }
            )
            points.append(point)

        self.client.upsert(
            collection_name=self.collection_name,
            points=points
        )

        if self.verbose:
            print(f"‚úÖ Texto '{source}' adicionado ({len(chunks)} chunks)")

    def get_stats(self) -> Dict:
        """Retorna estat√≠sticas da base de conhecimento"""
        try:
            categories = self.get_categories()

            # Conta documentos por categoria
            category_counts = {}
            for cat in categories:
                results = self.client.scroll(
                    collection_name=self.collection_name,
                    scroll_filter={"must": [{"key": "category", "match": {"value": cat}}]},
                    limit=1,
                    with_payload=False,
                    with_vectors=False
                )
                # Estima baseado no scroll
                category_counts[cat] = len(results[0]) if results else 0

            return {
                "total_documents": self.count(),
                "categories": categories,
                "category_counts": category_counts,
                "embedding_model": self.embedding_model.get_sentence_embedding_dimension(),
                "collection_name": self.collection_name
            }
        except Exception as e:
            return {"error": str(e)}

    def close(self):
        """Fecha explicitamente o cliente Qdrant"""
        try:
            if hasattr(self, 'client') and self.client:
                self.client.close()
                if self.verbose:
                    print("üîí Cliente Qdrant fechado")
        except Exception as e:
            if self.verbose:
                print(f"‚ö†Ô∏è Erro ao fechar cliente Qdrant: {e}")


def create_rag_instance(
    knowledge_base_dir: str = "./base_conhecimento",
    verbose: bool = True
) -> Optional[QdrantRAG]:
    """
    Factory function para criar inst√¢ncia do RAG com tratamento de erros

    Args:
        knowledge_base_dir: Pasta com documentos
        verbose: Mostrar logs

    Returns:
        Inst√¢ncia do RAG ou None se falhar
    """
    try:
        rag = QdrantRAG(
            knowledge_base_dir=knowledge_base_dir,
            auto_load=True,
            verbose=verbose
        )
        return rag
    except Exception as e:
        if verbose:
            print(f"‚ùå Erro ao inicializar RAG: {e}")
            print("üí° Certifique-se de instalar: pip install qdrant-client sentence-transformers PyPDF2")
        return None
